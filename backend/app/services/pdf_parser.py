"""
PDF parsing service using pdfplumber
Extracts text, metadata, and structure from research papers
"""

import pdfplumber
import re
import os
import asyncio
from typing import Dict, List, Tuple
from pathlib import Path
from ..services.blob_storage import download_from_blob, is_blob_url


class PDFParser:
    def __init__(self):
        pass

    async def parse_pdf(self, file_path: str) -> Dict[str, any]:
        """
        Parse PDF file and extract content, metadata, and structure
        Supports both local file paths and Vercel Blob URLs
        """
        temp_file_path = None
        try:
            # If it's a blob URL, download it first
            if is_blob_url(file_path):
                temp_file_path = await download_from_blob(file_path)
                if not temp_file_path:
                    return {
                        "title": "",
                        "authors": [],
                        "abstract": "",
                        "content": "",
                        "page_count": 0,
                        "metadata": {},
                        "success": False,
                        "error": "Failed to download PDF from blob storage",
                    }
                actual_file_path = temp_file_path
            else:
                actual_file_path = file_path
            
            # Open PDF document
            with pdfplumber.open(actual_file_path) as pdf:
                # Extract metadata
                metadata = pdf.metadata or {}

                # Extract text content
                full_text = ""
                page_texts = []

                for page in pdf.pages:
                    page_text = page.extract_text() or ""
                    page_texts.append(page_text)
                    full_text += page_text + "\n"

            # Extract title, authors, and abstract
            title, authors, abstract = self._extract_paper_metadata(full_text, metadata)

            # Clean the full text
            cleaned_text = self._clean_text(full_text)

            result = {
                "title": title,
                "authors": authors,
                "abstract": abstract,
                "content": cleaned_text,
                "page_count": len(page_texts),
                "metadata": metadata,
                "success": True,
            }
            
            # Clean up temporary file if we downloaded from blob
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.remove(temp_file_path)
                    print(f"[PDF_PARSER] Cleaned up temporary file: {temp_file_path}")
                except Exception as e:
                    print(f"[PDF_PARSER] Warning: Failed to delete temp file: {e}")
            
            return result

        except Exception as e:
            print(f"âœ— Error parsing PDF: {e}")
            # Clean up temporary file on error
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.remove(temp_file_path)
                except:
                    pass
            return {
                "title": "",
                "authors": [],
                "abstract": "",
                "content": "",
                "page_count": 0,
                "metadata": {},
                "success": False,
                "error": str(e),
            }

    def _extract_paper_metadata(
        self, text: str, pdf_metadata: dict
    ) -> Tuple[str, List[str], str]:
        """
        Extract title, authors, and abstract from paper text
        """
        lines = text.split("\n")
        clean_lines = [line.strip() for line in lines if line.strip()]

        # Extract title - usually one of the first few substantial lines
        title = ""
        if pdf_metadata.get("Title"):
            title = pdf_metadata["Title"]
        elif pdf_metadata.get("title"):
            title = pdf_metadata["title"]
        else:
            # Look for title in first 10 lines
            for line in clean_lines[:10]:
                if len(line) > 20 and len(line) < 200:
                    # Skip common headers/footers
                    if not any(
                        skip in line.lower()
                        for skip in [
                            "page",
                            "doi:",
                            "arxiv:",
                            "abstract",
                            "introduction",
                        ]
                    ):
                        title = line
                        break

        # Extract authors
        authors = []
        if pdf_metadata.get("Author"):
            authors = [pdf_metadata["Author"]]
        elif pdf_metadata.get("author"):
            authors = [pdf_metadata["author"]]
        else:
            # Look for author patterns in first 20 lines
            for i, line in enumerate(clean_lines[:20]):
                if self._looks_like_authors(line):
                    authors = self._parse_authors(line)
                    break

        # Extract abstract
        abstract = ""
        abstract_start = -1

        # Find abstract section
        for i, line in enumerate(clean_lines):
            if line.lower().startswith("abstract"):
                abstract_start = i
                break

        if abstract_start > -1:
            # Extract abstract content
            abstract_lines = []
            for i in range(
                abstract_start + 1, min(abstract_start + 20, len(clean_lines))
            ):
                line = clean_lines[i]
                # Stop at next section
                if any(
                    section in line.lower()
                    for section in ["introduction", "1.", "keywords", "index terms"]
                ):
                    break
                if len(line) > 10:
                    abstract_lines.append(line)

            abstract = " ".join(abstract_lines)[:500]  # Limit abstract length

        return title, authors, abstract

    def _looks_like_authors(self, line: str) -> bool:
        """
        Check if a line looks like it contains author names
        """
        # Common patterns for author lines
        author_patterns = [
            r"\b[A-Z][a-z]+ [A-Z][a-z]+\b",  # First Last
            r"\b[A-Z]\. [A-Z][a-z]+\b",  # F. Last
            r"\b[A-Z][a-z]+, [A-Z][a-z]+\b",  # Last, First
        ]

        for pattern in author_patterns:
            if re.search(pattern, line):
                return True

        return False

    def _parse_authors(self, author_line: str) -> List[str]:
        """
        Parse author names from a line
        """
        # Common separators
        separators = [",", " and ", "&", ";"]

        authors = [author_line]  # Default to whole line

        for sep in separators:
            if sep in author_line:
                authors = [author.strip() for author in author_line.split(sep)]
                break

        # Clean and filter authors
        cleaned_authors = []
        for author in authors:
            author = author.strip()
            # Remove common non-name elements
            if (
                author
                and len(author) > 2
                and not any(
                    skip in author.lower()
                    for skip in ["university", "department", "email", "@"]
                )
            ):
                cleaned_authors.append(author)

        return cleaned_authors[:5]  # Limit to 5 authors

    def _clean_text(self, text: str) -> str:
        """
        Clean extracted text for better processing
        """
        # Remove extra whitespace
        text = re.sub(r"\s+", " ", text)

        # Remove page headers/footers (simple heuristic)
        lines = text.split("\n")
        cleaned_lines = []

        for line in lines:
            line = line.strip()
            # Skip very short lines that are likely artifacts
            if len(line) < 3:
                continue
            # Skip lines that look like page numbers or headers
            if re.match(r"^\d+$", line) or re.match(r"^Page \d+", line):
                continue
            cleaned_lines.append(line)

        cleaned_text = "\n".join(cleaned_lines)

        # Limit text length for API efficiency
        if len(cleaned_text) > 10000:
            cleaned_text = cleaned_text[:10000] + "..."

        return cleaned_text

    async def validate_pdf(self, file_path: str) -> Tuple[bool, str]:
        """
        Validate that the file is a readable PDF
        Supports both local file paths and Vercel Blob URLs
        """
        temp_file_path = None
        try:
            # If it's a blob URL, download it first
            if is_blob_url(file_path):
                temp_file_path = await download_from_blob(file_path)
                if not temp_file_path:
                    return False, "Failed to download PDF from blob storage"
                actual_file_path = temp_file_path
            else:
                if not Path(file_path).exists():
                    return False, "File does not exist"
                actual_file_path = file_path

            with pdfplumber.open(actual_file_path) as pdf:
                page_count = len(pdf.pages)

            if page_count == 0:
                # Clean up temp file
                if temp_file_path and os.path.exists(temp_file_path):
                    try:
                        os.remove(temp_file_path)
                    except:
                        pass
                return False, "PDF has no pages"

            result = True, f"Valid PDF with {page_count} pages"
            
            # Clean up temporary file if we downloaded from blob
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.remove(temp_file_path)
                except:
                    pass
            
            return result

        except Exception as e:
            # Clean up temp file on error
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.remove(temp_file_path)
                except:
                    pass
            return False, f"Invalid PDF: {str(e)}"
